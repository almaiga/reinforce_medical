# Simple Setup Guide

## âœ… What You Need

- RTX PRO 6000 GPU (96GB VRAM) âœ…
- Python 3.8+
- CUDA installed

## ğŸš€ Quick Setup (3 Steps)

### Step 1: Install Dependencies (5 min)

```bash
# Install OpenRLHF (official package)
pip install openrlhf

# Install other requirements
pip install -r requirements.txt
```

### Step 2: Download Model & Generate Data (25 min)

```bash
# Run the quick start script
./quick_start.sh
```

This will:
- Download your fine-tuned model (Abdine/qwen3-4b-medical-selfplay-sft)
- Generate 638 training samples
- Verify everything is ready

### Step 3: Train! (1-2 hours)

```bash
# Launch training
./launch_training.sh
```

Done! âœ…

---

## ğŸ“Š What Happens During Training

1. **Loads models** on your GPU
   - Training model: Qwen3-4B (your fine-tuned)
   - Judge model: MedGemma-4B (local)

2. **Runs self-play games**
   - Attacker introduces errors
   - Assessor detects errors
   - Both improve through REINFORCE++

3. **Saves checkpoints** every 50 steps
   - Location: `checkpoints/medical_selfplay_RL_<timestamp>/`

---

## ğŸ” Monitoring

```bash
# Watch GPU usage
watch -n 1 nvidia-smi

# View training logs
tail -f checkpoints/medical_selfplay_RL_*/logs/training.log
```

Expected:
- GPU memory: 50-60GB / 96GB
- GPU utilization: 85-95%
- Training time: 1-2 hours

---

## â“ Why Not Install from selfplay-redteaming-reference?

Good question! Here's why:

### We Use Official OpenRLHF
```bash
pip install openrlhf  # âœ… Official package from PyPI
```

### selfplay-redteaming-reference is Just Reference
- It's there for **understanding** the Self-RedTeam paper
- We **adapted** their approach for medical domain
- Our code is in `medical_team/` - ready to use!
- No need to install their fork

### Our Implementation
- `medical_team/` - All our medical components
- `medical_team/local_reward_function.py` - Local judge
- Works with official OpenRLHF package
- Simpler and cleaner!

---

## ğŸ“ What You Have

```
medical_reward_0/
â”œâ”€â”€ medical_team/              â† Our medical components (ready!)
â”‚   â”œâ”€â”€ local_reward_function.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_medical_reinforce.sh  â† Training script
â”‚   â”œâ”€â”€ create_rl_training_data.py
â”‚   â””â”€â”€ convert_to_openrlhf_format.py
â”œâ”€â”€ selfplay-redteaming-reference/  â† Just for reference
â”‚   â””â”€â”€ (not used for training)
â”œâ”€â”€ quick_start.sh             â† Setup script
â””â”€â”€ launch_training.sh         â† Training launcher
```

---

## âœ… Checklist

Before training:

- [ ] OpenRLHF installed (`pip install openrlhf`)
- [ ] Model downloaded (run `./quick_start.sh`)
- [ ] Data generated (638 samples)
- [ ] GPU available (`nvidia-smi` shows RTX PRO 6000)

Then:
```bash
./launch_training.sh
```

---

## ğŸ‰ That's It!

No complex setup, no installing from local folders, just:

1. `pip install openrlhf`
2. `./quick_start.sh`
3. `./launch_training.sh`

Training will complete in 1-2 hours on your RTX PRO 6000! ğŸš€
