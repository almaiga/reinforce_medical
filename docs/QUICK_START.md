# Quick Start: Medical Self-Play Training

## ğŸ¯ Goal
Train models to detect medical errors using adversarial self-play, adapted from the [Self-RedTeam](https://github.com/mickelliu/selfplay-redteaming) paper.

## âœ… What You Have

Your codebase already has **two complete implementations**:

1. **TRL-based** (`script/selfplay/`) - âœ… Working, tested, recommended
2. **OpenRLHF-compatible** (`medical_team/`) - âœ… Complete, ready to use

## ğŸš€ Quick Start (2 Steps)

### Step 1: Test Everything Works
```bash
python3 tests/test_integration.py
```

### Step 2: Run Your Existing Training
```bash
python3 script/train_selfplay_advanced.py \
    --model_id Qwen/Qwen2.5-3B-Instruct \
    --judge_model_id google/medgemma-4b-it \
    --num_samples 400 \
    --max_rounds 10
```

**That's it!** Your existing code already implements Self-RedTeam.

### Optional: Remote Judge Server
If you want to run judge separately:
```bash
# Terminal 1: Start judge
python3 scripts/serve_medical_judge.py --model google/medgemma-4b-it

# Terminal 2: Run training (modify to use remote judge)
```

## ğŸ“Š What Happens During Training

```
Episode 1:
  â”œâ”€ Load 400 medical notes (100 per game type)
  â”œâ”€ Attacker: Introduce/modify errors
  â”œâ”€ Assessor: Classify notes (Safe/Harmful)
  â”œâ”€ Judge: Evaluate correctness
  â”œâ”€ Compute rewards (zero-sum)
  â””â”€ Update models

Episode 2:
  â”œâ”€ Models are now better
  â”œâ”€ Attacker creates harder errors
  â”œâ”€ Assessor gets better at detection
  â””â”€ Co-evolution continues...
```

## ğŸ® Game Types (4-Way Structure)

| Type | Source | Task | Difficulty | % |
|------|--------|------|------------|---|
| vanilla_harmful | Error note | Copy as-is | EASY | 25% |
| adversarial_harmful | Error note | Modify error | HARD | 25% |
| vanilla_benign | Clean note | Copy as-is | EASY | 25% |
| adversarial_benign | Clean note | Inject error | HARD | 25% |

## ğŸ’° Rewards (Zero-Sum)

**Attacker wins when:**
- Errors go undetected by assessor
- Errors are realistic
- Respects game category

**Assessor wins when:**
- Correctly detects errors
- Correctly rejects clean notes
- Provides good explanations

**Zero-sum property:**
```
Attacker_reward + Assessor_reward â‰ˆ 0
```

## ğŸ§ª Testing

### Test Integration
```bash
python3 tests/test_integration.py
```

### Test Judge Server
```bash
# Terminal 1: Start server
python3 scripts/serve_medical_judge.py

# Terminal 2: Test connection
python3 -m medical_team.remote_judge --url http://localhost:8000
```

### Test Data Loading
```bash
python3 scripts/prepare_medical_data.py --num-samples 40 --no-save
```

## ğŸ“ Key Files

```
script/selfplay/              # â­ YOUR WORKING IMPLEMENTATION
â”œâ”€â”€ main.py                   # TRL training (USE THIS)
â”œâ”€â”€ data.py                   # Data loading (4-way structure)
â”œâ”€â”€ rewards.py                # Reward calculation (zero-sum)
â”œâ”€â”€ judge.py                  # Judge evaluation
â””â”€â”€ prompts.py                # Prompt generation

medical_team/                 # OpenRLHF-compatible (optional)
â”œâ”€â”€ medical_game_manager.py   # Game orchestration
â”œâ”€â”€ utils.py                  # Reward functions
â”œâ”€â”€ prompts.py                # Prompt templates
â”œâ”€â”€ remote_judge.py           # Judge client
â””â”€â”€ README.md                 # Documentation

scripts/                      # New additions (optional)
â”œâ”€â”€ prepare_medical_data.py   # Data preparation wrapper
â”œâ”€â”€ serve_medical_judge.py    # Judge HTTP server
â””â”€â”€ train_medical_selfplay_simple.py  # Demo script
```

**Note:** Your `script/selfplay/` is your main implementation. The new `scripts/` and `medical_team/` are optional additions for OpenRLHF compatibility.

## ğŸ“ Understanding Self-RedTeam

The Self-RedTeam paper's key insights:

1. **Online Self-Play**: Models co-evolve, not static training
2. **Zero-Sum Game**: Attacker vs Assessor competition
3. **Nash Equilibrium**: Converges to robust safety
4. **Hidden CoT**: Private reasoning improves diversity

Your adaptation:
- âœ… Safety â†’ Medical error detection
- âœ… Jailbreaking â†’ Error introduction
- âœ… WildGuard â†’ MedGemma judge
- âœ… 2-way â†’ 4-way game structure

## ğŸ’¡ Recommendations

### For Single GPU (RTX 6000):
1. âœ… **Use your TRL approach** - It works great!
2. âœ… **Run judge on same GPU** - 4B model fits fine
3. âŒ **Skip OpenRLHF Ray** - Unnecessary complexity

### For Multi-GPU Cluster:
1. Consider OpenRLHF Ray integration
2. Use `medical_team/` components
3. Adapt Self-RedTeam's `train_ppo_ray.py`

## ğŸ› Troubleshooting

### Judge Server Won't Start
```bash
# Check if port is in use
lsof -i :8000

# Try different port
python3 scripts/serve_medical_judge.py --port 8001
```

### Out of Memory
```bash
# Reduce batch size
--rollout_batch_size 8

# Use smaller model
--model Qwen/Qwen2.5-1.5B-Instruct
```

### Data Loading Fails
```bash
# Check MEDEC path
ls data_copy/MEDEC/MEDEC-MS/

# Try smaller sample
--num-samples 40
```

## ğŸ“š Documentation

- `medical_team/README.md` - Component documentation
- `scripts/README_SELFPLAY.md` - Training guide
- `IMPLEMENTATION_STATUS.md` - What's complete
- `tests/test_integration.py` - Integration tests

## ğŸ‰ You're Ready!

Everything is set up and tested. Just run:

```bash
# Quick test (5 minutes)
python3 tests/test_integration.py

# Full training (hours)
python3 script/train_selfplay_advanced.py --num_samples 400
```

Good luck with your medical self-play training! ğŸš€
